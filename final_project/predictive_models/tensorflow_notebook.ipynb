{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec0fb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal of this file is to predict temperature for all 9 stations for all 24 hours for 7 days\n",
    "\n",
    "# We also want to include the n\n",
    "# on-time dependent variables, i.e., include variables that indicate the station number (locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "676216b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Dropout\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded33c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data\n",
    "temps = pd.read_excel(r\"C:\\Users\\Matthew\\PycharmProjects\\ISDS-7075-Project\\final_project\\datasets\\dataset.xlsx\",\n",
    "                      sheet_name=\"temperature_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b8f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables for each of the 9 weather stations\n",
    "temps = pd.get_dummies(temps, columns=[\"station_id\"])\n",
    "\n",
    "# Creating a date field\n",
    "temps[\"date\"] = pd.to_datetime(temps[[\"year\", \"month\", \"day\"]], format='%Y-%m-%d', errors='coerce')\n",
    "train_dates = temps[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying which cols for training (h1 is the y in this example)\n",
    "cols = list(temps)[4:-1]\n",
    "\n",
    "\n",
    "# New df with only x vars\n",
    "df_for_training = temps[cols].astype(float)\n",
    "df_for_training = df_for_training.iloc[0:14715,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LTSM uses sigmoid and tanh that are sensitive to magnitude, so values need to be normalized\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(df_for_training)\n",
    "df_for_training_scaled = scaler.transform(df_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4087776",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_future = 1\n",
    "n_past = 14\n",
    "\n",
    "\n",
    "\n",
    "for i in range(n_past, len(df_for_training_scaled) - n_future + 1):\n",
    "    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "trainX, trainY = np.array(trainX), np.array(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Autoencoder model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(trainY.shape[1]))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = model.fit(x=trainX, y=trainY, epochs=10, batch_size=16, validation_split=0.1, verbose=1)\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
